{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.3"
    },
    "colab": {
      "name": "Project 1_Fall 2021.ipynb",
      "private_outputs": true,
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rskim47/BigDataDistributedProgramming/blob/master/Project_1_Fall_2021.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y0QaNuzDJRdA"
      },
      "source": [
        "# <p style=\"text-align: center;\">MIS 285N: Big Data and Distributed Programming</p>\n",
        "# <p style=\"text-align: center;\">Project - 1 : Apache Spark</p>\n",
        "## <p style=\"text-align: center;\">Instructor: Dr. Ramesh Yerraballi</p>\n",
        "## <p style=\"text-align: center;\">Due: Tuesday, September 14th submitted via Canvas by 11:59 pm</p>\n",
        "\n",
        "Your work should be written in a **Jupyter notebook**.   \n",
        "\n",
        "Also, please make sure your code runs in your notebook before submitting.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "This project is based on Map-Reduce Framework. In these you will get to work with Spark and will get to know how \n",
        "does spark work, what functionalities does spark provide, what does map-reduce framework do and why is it useful. \n",
        "\n",
        "In this project you will be implementing a basic song recommender system. You will be given a dataset where there are multiple csv files. These csv files have data corresponding to song play count and song information.\n",
        "\n",
        "The data you would be using will be provided in a zip file along with this notebook. The __msd.zip__ archive contains:\n",
        "1. **'kaggle_visible_evaluation_triplets.txt'**. We will be using the visible part of the testing data to understand the working on Apache Spark.  The user's listening history is provided as: (user, song, play count).  \n",
        "2. In **'kaggle_songs.txt'** file, each song is marked using an index for easier representation of songs.  \n",
        "3. And **'kaggle_users.txt'** file is the canonical list of user identifiers.\n",
        "4. Take **'MSDChallengeGettingstarted.pdf'** as your reference.\n",
        "\n",
        "\n",
        "\n",
        "### **What to turn in?**  \n",
        "\n",
        "A zip folder which will have:\n",
        "1. Jupyter Notebook\n",
        "2. A brief report in PDF format on what features you used for recommendation. And a brief explanation of flow of your code. For example,  what RDD does what or, why it was created.\n",
        "3. datasets folder with the csv files you are using in your notebook.\n",
        "4. Notebook should use relative path to the csv files in datasets folder.\n",
        "5. Name of the zip folder - `<your_name>_<your_partner_name>.zip`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mCB_7umAOLdT"
      },
      "source": [
        "# Setting up environment\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://archive.apache.org/dist/spark/spark-3.0.0/spark-3.0.0-bin-hadoop3.2.tgz\n",
        "!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPeVAWx8S7Ya"
      },
      "source": [
        "import os\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\"\n",
        "!pip install -q findspark\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4W-7Q98JRdE"
      },
      "source": [
        "This project consists of 4 questions:  \n",
        "\n",
        "1. Create an RDD with _kaggle_visible_evaluation_triplets.txt_ and replace the song name with the song index from _kaggle_songs.txt_. Identify the number of songs that do not have any rating. \n",
        "2. Generate song ratings based on the song play count as a normalized score between 0 and 1. \n",
        "3. Identify the popular song based on this rating and recommend songs to user, given user id based on the algorithm used in Movie recommender system from class. \n",
        "4. Using Cosine similarity function, identify pair-wise similarity between each pair of users and generate the top 5 most similar users without an overlap in users. \n",
        "\n",
        "The above list is the higer level idea about the questions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6pHJRuZ6JRdF"
      },
      "source": [
        "### Starter code ####\n",
        "import findspark\n",
        "print(findspark.find())\n",
        "findspark.init() #May need to change based on machine\n",
        "from pyspark import SparkConf, SparkContext\n",
        "conf = SparkConf().setMaster(\"local[*]\").setAppName(\"Songs\")\n",
        "sc = SparkContext(conf = conf)\n",
        "#### These lines are to tell jupyter where to find Apache Spark ####"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-QjUEqB8JRdH"
      },
      "source": [
        "#sc.stop()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sMfgVcz-JRdI"
      },
      "source": [
        "## Read triplet file into RDD\n",
        "triplet_rdd = sc.textFile(r\"kaggle_visible_evaluation_triplets.txt\") \\\n",
        "    .map(lambda line: line.split(\"\\t\")) \n",
        "\n",
        "triplet_rdd.take(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QzvUl1-oz4b7"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8lyZ32CXJRdJ"
      },
      "source": [
        "## Step 1: \n",
        "Replace song name with song index and identify the number of songs without user history"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XD2Y68RLgus"
      },
      "source": [
        "songs_rdd = sc.textFile(r\"kaggle_songs.txt\") \\\n",
        "    .map(lambda line: line.split()) \n",
        "\n",
        "print(songs_rdd.take(5))\n",
        "\n",
        "#1- Replace Song name with Index \n",
        "joined_rdd=triplet_rdd.map(lambda x: (x[1],[x[0],x[2]])).join(songs_rdd)\n",
        "print(joined_rdd.take(5)) #reordered column\n",
        "final_rdd=joined_rdd.map(lambda x: x[1])\n",
        "print(final_rdd.take(5)) \n",
        "refinal_rdd=final_rdd.map(lambda x: (x[0][0],int(x[0][1]),x[1]))\n",
        "print(refinal_rdd.take(5))\n",
        "\n",
        "#2- Identify Songs without history\n",
        "nodupes_rdd=refinal_rdd.map(lambda x: x[2])\n",
        "nodupes_rdd=nodupes_rdd.distinct()\n",
        "print(nodupes_rdd.take(10))\n",
        "x=nodupes_rdd.count()\n",
        "y=songs_rdd.count()\n",
        "nohistory_songs=y-x\n",
        "print(nohistory_songs)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SqniTLiPJRdK"
      },
      "source": [
        "## Step 2:\n",
        "Generate song ratings based on the play_count. For example, if (song_1, 5; song_2, 10; song_3, 5) i.e., song_1 is played 5 times, song_2 is played 10 times and song_3 is played 5 times, the normalized rating score should be 0.25, 0.5 and 0.25 respectively. \n",
        "Similarly, generate the rating for all the songs. You may notice that based on all songs, the rating is almost always very low. So, think of the best way to convert song count to ratings. (Hint: Try generating ratings based on each user's song play history)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUuEbWRusJst"
      },
      "source": [
        "#1- Play counts\n",
        "plays_rdd=refinal_rdd.map(lambda x: (x[2],x[1]))\n",
        "plays_rdd=plays_rdd.reduceByKey(lambda x,y: x+y)\n",
        "print(plays_rdd.take(5))\n",
        "\n",
        "#2- Ratings\n",
        "user_rdd=refinal_rdd.map(lambda x: (x[0],x[1]))\n",
        "user_rdd=user_rdd.reduceByKey(lambda x,y: x+y)\n",
        "#print(user_rdd.take(5))\n",
        "\n",
        "ratings_rdd=refinal_rdd.map(lambda x: (x[0],[x[1],x[2]])).join(user_rdd)\n",
        "#print(ratings_rdd.take(5))\n",
        "ratings_rdd=ratings_rdd.map(lambda x: (x[0],x[1][0][0],x[1][0][1],x[1][1]))\n",
        "\n",
        "ratings_rdd=ratings_rdd.map(lambda x: (x[2],x[1]/x[3]))\n",
        "#print(ratings_rdd.take(5))\n",
        "\n",
        "total_rdd=ratings_rdd.groupByKey().map(lambda x : (x[0], list(x[1])))\n",
        "print(total_rdd.take(5))\n",
        "overallratings_rdd = total_rdd.map(lambda x : (x[0],sum(x[1]) / len(x[1])))\n",
        "print(overallratings_rdd.take(5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76mO-AqSJRdL"
      },
      "source": [
        "## Step 3: \n",
        "For a given user_id (choose one by yourselves), rating, recommend 5 other songs from the list. One way to do this is based on another user who liked the same song liked by this user with rating more than the given rating and recommend the 5 songs based on the matched user's rating. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tr6C2tJpJRdM"
      },
      "source": [
        "## Step 4: \n",
        "1. Compute cosine similarity between all pairs of users. \n",
        "2. Sort the similarity score and print the top-5 similar users. \n",
        "3. If the top-5 user set has an user appearing more than once, ignore that pair and take the next best pair from the sorted list. \n",
        "4. For a given user_id, identify the top-5 similar users and hence song recommendations from other user's list. "
      ]
    }
  ]
}